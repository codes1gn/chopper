Init unique module name
------ PYTHON SRC -------
[91m
*****dumping python ast****
Module(body=[FunctionDef(
  name='forward',
  args=arguments(
    args=[
      arg(
        arg='self',
        annotation=None),
      arg(
        arg='a',
        annotation=None),
      arg(
        arg='b',
        annotation=None)],
    vararg=None,
    kwonlyargs=[],
    kw_defaults=[],
    kwarg=None,
    defaults=[]),
  body=[
    Assign(
      targets=[Name(
        id='c',
        ctx=Store())],
      value=BinOp(
        left=Name(
          id='a',
          ctx=Load()),
        op=Mult(),
        right=Name(
          id='b',
          ctx=Load()))),
    Assign(
      targets=[Name(
        id='e',
        ctx=Store())],
      value=BinOp(
        left=Name(
          id='c',
          ctx=Load()),
        op=Mult(),
        right=Name(
          id='b',
          ctx=Load()))),
    Return(value=Name(
      id='e',
      ctx=Load()))],
  decorator_list=[
    Call(
      func=Name(
        id='backend',
        ctx=Load()),
      args=[Str(s='IREE')],
      keywords=[]),
    Call(
      func=Name(
        id='annotate_arguments',
        ctx=Load()),
      args=[List(
        elts=[
          NameConstant(value=None),
          Tuple(
            elts=[
              Name(
                id='shape',
                ctx=Load()),
              Attribute(
                value=Name(
                  id='torch',
                  ctx=Load()),
                attr='float32',
                ctx=Load())],
            ctx=Load()),
          Tuple(
            elts=[
              Name(
                id='shape',
                ctx=Load()),
              Attribute(
                value=Name(
                  id='torch',
                  ctx=Load()),
                attr='float32',
                ctx=Load())],
            ctx=Load())],
        ctx=Load())],
      keywords=[])],
  returns=None)])[0m[95m
*****dumping python code*****


@backend('IREE')
@annotate_arguments([None, (shape, torch.float32), (shape, torch.float32)])
def forward(self, a, b):
    c = (a * b)
    e = (c * b)
    return e
[0m
*******************

_pass_table == >  {'IdenticalPastPass': '0', 'StatementConversionPass': '1', 'AnnotateCompletionPass': '2'}
pass_manager_base::schedule_passes

====== enter AnnotateCompletionPass =====

<chopper.pass_manager.transformers.annotate_types_visitor.AnnotateTypesVisitor object at 0x7fbf41ee72b0> ::visit_FunctionDef

ValueBuilder.create forward symbol: name = a, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
insert symbol into autodiff_symbol_table :  a
ValueBuilder.create backward symbol: name = a, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create savedact symbol: name = a, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create funret symbol: name = a, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create forward symbol: name = b, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
insert symbol into autodiff_symbol_table :  b
ValueBuilder.create backward symbol: name = b, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create savedact symbol: name = b, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create funret symbol: name = b, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
<chopper.pass_manager.transformers.annotate_types_visitor.AnnotateTypesVisitor object at 0x7fbf41ee72b0> ::visit_Assign

warning: redefine of value a with, newtype = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
warning: redefine of value b with, newtype = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create forward symbol: name = c, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
insert symbol into autodiff_symbol_table :  c
ValueBuilder.create backward symbol: name = c, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
<chopper.pass_manager.transformers.annotate_types_visitor.AnnotateTypesVisitor object at 0x7fbf41ee72b0> ::visit_Assign

ValueBuilder.create savedact symbol: name = c, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
warning: redefine of value b with, newtype = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
ValueBuilder.create forward symbol: name = e, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
insert symbol into autodiff_symbol_table :  e
ValueBuilder.create backward symbol: name = e, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
<chopper.pass_manager.transformers.annotate_types_visitor.AnnotateTypesVisitor object at 0x7fbf41ee72b0> ::visit_Return

ValueBuilder.create funarg symbol: name = e, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
>=============================<
>==== Forward ValueSymbolTable Summary ====<
>=============================<

Count of Symbol Entries = 4
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #2 
=> SsaId(value='c', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #3 
=> SsaId(value='e', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Forward ValueSymbolTable Summary ==<
>=============================<

>=============================<
>==== Autodiff ValueSymbolTable Summary ====<
>=============================<

Count of Symbol Entries = 4
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #2 
=> SsaId(value='c', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #3 
=> SsaId(value='e', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Autodiff ValueSymbolTable Summary ==<
>=============================<

>=============================<
>==== Saved Activation Table Summary ====<
>=============================<

Count of Symbol Entries = 3
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a-act', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b-act', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #2 
=> SsaId(value='c-act', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Saved Activation Table Summary ==<
>=============================<

>=============================<
>==== Autodiff Func Args Table Summary ====<
>=============================<

Count of Symbol Entries = 1
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='e', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Autodiff Func Args Table Summary ==<
>=============================<

>=============================<
>==== Autodiff Func Rets Table Summary ====<
>=============================<

Count of Symbol Entries = 2
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Autodiff Func Rets Table Summary ==<
>=============================<

pass_manager_base::schedule_passes

====== enter StatementConversionPass =====


====== enter StatementConversionPass - Transformer = <class 'chopper.pass_manager.transformers.stmt_node_mapping_transformer.StmtNodeMappingTransformer'> =====

<chopper.pass_manager.transformers.stmt_node_mapping_transformer.StmtNodeMappingTransformer object at 0x7fbf41ee73c8> ::visit_Module

<chopper.pass_manager.transformers.stmt_node_mapping_transformer.StmtNodeMappingTransformer object at 0x7fbf41ee73c8> ::visit_FunctionDef

<chopper.pass_manager.transformers.stmt_node_mapping_transformer.StmtNodeMappingTransformer object at 0x7fbf41ee73c8> ::visit_Assign

insert symbol into autodiff_symbol_table :  a-0
ValueBuilder.create backward symbol: name = a-0, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
insert symbol into autodiff_symbol_table :  b-0
ValueBuilder.create backward symbol: name = b-0, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
<chopper.pass_manager.transformers.stmt_node_mapping_transformer.StmtNodeMappingTransformer object at 0x7fbf41ee73c8> ::visit_Assign

insert symbol into autodiff_symbol_table :  c-0
ValueBuilder.create backward symbol: name = c-0, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
insert symbol into autodiff_symbol_table :  b-1
ValueBuilder.create backward symbol: name = b-1, type = RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))
<chopper.pass_manager.transformers.stmt_node_mapping_transformer.StmtNodeMappingTransformer object at 0x7fbf41ee73c8> ::visit_Return


====== enter StatementConversionPass - Transformer = <class 'chopper.pass_manager.transformers.autodiff_merge_replicas.AutodiffMergeReplicas'> =====

<chopper.pass_manager.transformers.autodiff_merge_replicas.AutodiffMergeReplicas object at 0x7fbf41ee73c8> ::visit_FunctionDef.

<chopper.pass_manager.transformers.autodiff_merge_replicas.AutodiffMergeReplicas object at 0x7fbf41ee73c8> ::visit_Assign.

<chopper.pass_manager.transformers.autodiff_merge_replicas.AutodiffMergeReplicas object at 0x7fbf41ee73c8> ::visit_Assign.


====== enter StatementConversionPass - Transformer = <class 'chopper.pass_manager.transformers.stmt_conversion_ready_check_visitor.StmtConversionReadyCheckVisitor'> =====


====== enter StatementConversionPass - Transformer = <class 'chopper.pass_manager.transformers.stmt_fix_dependency_transformer.StmtFixDependencyTransformer'> =====

<chopper.pass_manager.transformers.stmt_fix_dependency_transformer.StmtFixDependencyTransformer object at 0x7fbf41ee73c8> Fix Transformer::handling visit_FunctionDef on node

<chopper.pass_manager.transformers.stmt_fix_dependency_transformer.StmtFixDependencyTransformer object at 0x7fbf41ee73c8> Fix handling visit_Module on node

Finally Symbol Table 
>=============================<
>==== Forward ValueSymbolTable Summary ====<
>=============================<

Count of Symbol Entries = 4
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #2 
=> SsaId(value='c', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #3 
=> SsaId(value='e', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Forward ValueSymbolTable Summary ==<
>=============================<

>=============================<
>==== Autodiff ValueSymbolTable Summary ====<
>=============================<

Count of Symbol Entries = 8
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #2 
=> SsaId(value='c', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #3 
=> SsaId(value='e', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #4 
=> SsaId(value='a-0', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #5 
=> SsaId(value='b-0', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #6 
=> SsaId(value='c-0', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #7 
=> SsaId(value='b-1', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Autodiff ValueSymbolTable Summary ==<
>=============================<

>=============================<
>==== Saved Activation Table Summary ====<
>=============================<

Count of Symbol Entries = 3
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a-act', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b-act', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #2 
=> SsaId(value='c-act', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Saved Activation Table Summary ==<
>=============================<

>=============================<
>==== Autodiff Func Args Table Summary ====<
>=============================<

Count of Symbol Entries = 1
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='e', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Autodiff Func Args Table Summary ==<
>=============================<

>=============================<
>==== Autodiff Func Rets Table Summary ====<
>=============================<

Count of Symbol Entries = 2
Listing Symbol Entries ...


Symbol #0 
=> SsaId(value='a', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))

Symbol #1 
=> SsaId(value='b', op_no=None) 
=> RankedTensorType(dimensions=[Dimension(value=2), Dimension(value=3)], element_type=FloatType(type=<FloatTypeEnum.f32: 'f32'>))


>=============================<
>== End Autodiff Func Rets Table Summary ==<
>=============================<

------ ATIR IR -------
module @forward_3aadccec829346bba932f5de1578565f {
  func @forward(%a: tensor<2x3xf32>, %b: tensor<2x3xf32>) -> (tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>) {
    %c = atir.mul %a , %b : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %e = atir.mul %c , %b : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %a-act = atir.identity %a : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %b-act = atir.identity %b : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %c-act = atir.identity %c : (tensor<2x3xf32>) -> tensor<2x3xf32>
    return %e, %a-act, %b-act, %c-act : tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>
  }
}
module @backward_3aadccec829346bba932f5de1578565f {
  func @bpfunction(%e: tensor<2x3xf32>, %a-act: tensor<2x3xf32>, %b-act: tensor<2x3xf32>, %c-act: tensor<2x3xf32>) -> (tensor<2x3xf32>, tensor<2x3xf32>) {
    %b-1 = atir.mul %c-act , %e : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %c-0 = atir.mul %e , %b-act : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %c = atir.identity %c-0 : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %b-0 = atir.mul %a-act , %c : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %a-0 = atir.mul %c , %b-act : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %a = atir.identity %a-0 : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %b = atir.add %b-1 , %b-0 : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    return %a, %b : tensor<2x3xf32>, tensor<2x3xf32>
  }
}
------ TOSA IR -------
module @forward_3aadccec829346bba932f5de1578565f  {
  func @forward(%arg0: tensor<2x3xf32>, %arg1: tensor<2x3xf32>) -> (tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>) {
    %0 = "tosa.mul"(%arg0, %arg1) {shift = 0 : i32} : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %1 = "tosa.mul"(%0, %arg1) {shift = 0 : i32} : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %2 = "tosa.identity"(%arg0) : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %3 = "tosa.identity"(%arg1) : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %4 = "tosa.identity"(%0) : (tensor<2x3xf32>) -> tensor<2x3xf32>
    return %1, %2, %3, %4 : tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>, tensor<2x3xf32>
  }
}


------ TOSA IR -------
module @backward_3aadccec829346bba932f5de1578565f  {
  func @bpfunction(%arg0: tensor<2x3xf32>, %arg1: tensor<2x3xf32>, %arg2: tensor<2x3xf32>, %arg3: tensor<2x3xf32>) -> (tensor<2x3xf32>, tensor<2x3xf32>) {
    %0 = "tosa.mul"(%arg3, %arg0) {shift = 0 : i32} : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %1 = "tosa.mul"(%arg0, %arg2) {shift = 0 : i32} : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %2 = "tosa.identity"(%1) : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %3 = "tosa.mul"(%arg1, %2) {shift = 0 : i32} : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %4 = "tosa.mul"(%2, %arg2) {shift = 0 : i32} : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    %5 = "tosa.identity"(%4) : (tensor<2x3xf32>) -> tensor<2x3xf32>
    %6 = "tosa.add"(%0, %3) : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<2x3xf32>
    return %5, %6 : tensor<2x3xf32>, tensor<2x3xf32>
  }
}


------ RESULTS in VULKAN GPU -------
vulkan backend inited
<built-in method items of BoundModules object at 0x7fbf41f593b8>
<class 'chopper.iree.runtime.system_api.BoundModule'>
<BoundModule <VmModule forward_3aadccec829346bba932f5de1578565f : [forward, __init]>>
_Callable_Func.apply args = (tensor([[0.5716, 0.3323, 0.4277],
        [0.5606, 0.0400, 0.1030]], requires_grad=True), tensor([[0.5578, 0.9599, 0.9025],
        [0.1396, 0.4644, 0.8907]], requires_grad=True)), kwargs = {}
execute forward, and inputs =  (tensor([[0.5716, 0.3323, 0.4277],
        [0.5606, 0.0400, 0.1030]], requires_grad=True), tensor([[0.5578, 0.9599, 0.9025],
        [0.1396, 0.4644, 0.8907]], requires_grad=True))
execute forward, and outputs =  [tensor([[0.1778, 0.3062, 0.3484],
        [0.0109, 0.0086, 0.0817]]), tensor([[0.5716, 0.3323, 0.4277],
        [0.5606, 0.0400, 0.1030]]), tensor([[0.5578, 0.9599, 0.9025],
        [0.1396, 0.4644, 0.8907]]), tensor([[0.3188, 0.3190, 0.3860],
        [0.0783, 0.0186, 0.0918]])]
save for backwards =  tensor([[0.5716, 0.3323, 0.4277],
        [0.5606, 0.0400, 0.1030]]) tensor([[0.5578, 0.9599, 0.9025],
        [0.1396, 0.4644, 0.8907]]) tensor([[0.3188, 0.3190, 0.3860],
        [0.0783, 0.0186, 0.0918]])
forward return =  tensor([[0.1778, 0.3062, 0.3484],
        [0.0109, 0.0086, 0.0817]])
reference result =
 tensor([[0.1778, 0.3062, 0.3484],
        [0.0109, 0.0086, 0.0817]], grad_fn=<MulBackward0>)
actual result =
 tensor([[0.1778, 0.3062, 0.3484],
        [0.0109, 0.0086, 0.0817]], grad_fn=<_Callable_FuncBackward>)
FF TEST RESULT = True
execute backward, and inputs =  tensor([[1., 1., 1.],
        [1., 1., 1.]])
ctx.saved_tensors =  (tensor([[0.5716, 0.3323, 0.4277],
        [0.5606, 0.0400, 0.1030]]), tensor([[0.5578, 0.9599, 0.9025],
        [0.1396, 0.4644, 0.8907]]), tensor([[0.3188, 0.3190, 0.3860],
        [0.0783, 0.0186, 0.0918]]))
execute backward, and outputs =  (array([[0.31110325, 0.9213654 , 0.8145207 ],
       [0.01949772, 0.21569392, 0.7933386 ]], dtype=float32), array([[0.6376842 , 0.63793916, 0.7719964 ],
       [0.15655449, 0.03719788, 0.18350545]], dtype=float32))
reference grad =  tensor([[0.3111, 0.9214, 0.8145],
        [0.0195, 0.2157, 0.7933]])
actual grad =  tensor([[0.3111, 0.9214, 0.8145],
        [0.0195, 0.2157, 0.7933]])
BP LHS TEST RESULT = True
BP RHS TEST RESULT = True
